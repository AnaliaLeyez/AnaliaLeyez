{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnaliaLeyez/AnaliaLeyez/blob/main/Copia_de_Semana_06_Tema_02_Spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [spacy](https://spacy.io/)\n",
        "\n",
        "\n",
        "Librería de Python para el procesamiento del lenguaje natural que esparticularmente rápido e intuitivo, proporciona modelos preentrenados en diferentes lenguajes, lo cual junto a una sintaxis clara hace que sea ideal para principiantes en el campo de la NLP.\n",
        "\n",
        "Los modelos pre entrenados utilizan la aquitectura de transformers."
      ],
      "metadata": {
        "id": "NYUSEvVx1Fs7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalación"
      ],
      "metadata": {
        "id": "pkCTAjVG4CGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "id": "H888ITrZ4HZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Versión"
      ],
      "metadata": {
        "id": "EI5uvpRI4bHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "spacy.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KaEL56zK4dSI",
        "outputId": "a17100ca-2b61-4e44-8118-3ae7697d1143"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.5.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# para generar nuestro propio lenguaje\n",
        "# from spacy.tokenizer import Tokenizer\n",
        "# from spacy.lang.es import Spanish\n",
        "# nlp_spanish = Spanish()"
      ],
      "metadata": {
        "id": "PNmugwN9Y7q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Configuración y descarga de modelos](https://spacy.io/models)\n",
        "\n",
        "Por defecto viene configurada para trabajar en inglés por lo tanto debemos descargar o utilizar librerías de español.\n",
        "\n",
        "En nuestro caso vamos a descargar un modelo sencillo pero podemos utilizar otros más potentes.\n"
      ],
      "metadata": {
        "id": "P1Wy_Att5SqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# descargo el modelo\n",
        "!python -m spacy download es_core_news_sm\n",
        "\n",
        "# importo el modelo\n",
        "import spacy\n",
        "nlp_es_sm = spacy.load(\"es_core_news_sm\")"
      ],
      "metadata": {
        "id": "bDJGak8H5buL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Tokenizador y tokens](https://www.nltk.org/api/nltk.tokenize.html)\n",
        "\n",
        "Un tokenizador es una herramienta que se utiliza en el procesamiento del lenguaje natural para dividir un texto en palabras individuales o en frases más pequeñas llamadas tokens. El proceso de tokenización implica la separación de signos de puntuación y caracteres especiales y la separación de las palabras en un texto.\n",
        "\n",
        "Existen varios tipos de tokenizadores, algunos dividen las palabras en sub palabras.\n",
        "\n",
        "\n",
        "Algunas propiedas que son comunes a los string\n",
        "\n",
        "| Función | Descripción |\n",
        "| --- | --- |\n",
        "| lower_ | Devuelve el token en minúscula |\n",
        "| is_alpha | Contiene letras del alfabeto |\n",
        "| is_ascii | Posee todos caracteres ASCII (128) |\n",
        "| is_digit | Es un dígito |\n",
        "| is_lower | Está en minúscula? |\n",
        "| is_upper | Está en mayúscula? |\n",
        "| is_title | Está en título o capitalizado? |\n",
        "| is_punct | Es un caracter de puntuación? |\n",
        "| is_left_punct | Es un caracter de puntuación izquierdo? Por ejemplo ( [ { |\n",
        "| is_right_punct | Es un caracter de puntuación izquierdo? Por ejemplo ) ] } |\n",
        "| is_sent_start | Está comenzando una sentencia u oración? |\n",
        "| is_sent_end | Está finalizando una sentencia u oración? |\n",
        "| is_space | Es un espacio o tab |\n",
        "| is_bracket | Es un corchete? |\n",
        "| is_quote | Es una cita? |\n",
        "| is_currency | Es un simbolo de moneda? |\n",
        "| like_url | Es una url? |\n",
        "| like_num | Es un número? |\n",
        "| like_email | Es un email? |\n",
        "| is_oov | Está fuera del vocabulario del idioma? |\n",
        "| is_stop | Es un stop word? |"
      ],
      "metadata": {
        "id": "izDO1Y2a3L5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto = \"\"\"La inteligencia artificial (IA), en el contexto de las ciencias de la\n",
        "computación, es el conjunto de sistemas o combinación de algoritmos, cuyo\n",
        "propósito es crear máquinas que imitan la inteligencia humana para realizar\n",
        "tareas y pueden mejorar conforme la información que recopilan. La inteligencia\n",
        "artificial no tiene como finalidad reemplazar a los humanos, sino mejorar\n",
        "significativamente las capacidades y contribuciones humanas. Se hizo presente\n",
        "poco después de la Segunda Guerra Mundial, y el nombre lo acuñó en 1956 el\n",
        "informático John McCarthy, en la Conferencia de Dartmouth.\"\"\"\n",
        "# Fijense como separó las palabras, los caracteres especiales tienen su propio\n",
        "# lugar en la lista, no están pegadas a las palabras.\n",
        "\n",
        "# La función de tokenizador me devuelve algo del tipo Doc (https://spacy.io/api/doc)\n",
        "# El objeto Doc contiene un array de structs TokenC. Los objetos Token y Span a\n",
        "# nivel Python son vistas de esta matriz, es decir, no poseen los datos en sí.\n",
        "text_doc = nlp_es_sm(texto)"
      ],
      "metadata": {
        "id": "JzSklBKs3_io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vamos a ver los tokens\n",
        "print([t for t in text_doc])"
      ],
      "metadata": {
        "id": "SyZmm_opRZzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Propiedades de un token"
      ],
      "metadata": {
        "id": "3Rgq3JeqZ1Qu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Obteniendo sentencias u oraciones"
      ],
      "metadata": {
        "id": "Yr-7ifBdb7tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Para obtener las sentencias debemos pedirle al Doc la propiedad sents\n",
        "print([ sent for sent in text_doc.sents])"
      ],
      "metadata": {
        "id": "L9HNwAUub2c1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Obteniendo la palabra y la sentencia a la que pertenece"
      ],
      "metadata": {
        "id": "-NcTNLWccD4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Para obtener esto, debemos recorrer el Doc y mostrar la palabra y la sentencia relacionada\n",
        "# Notar que estmos mostrando una tupla donde el primer elemento es la palabra y\n",
        "# el segundo elemento es la sentencia a la que pertenece esa palabra.\n",
        "print([ (word, word.sent) for word in text_doc])"
      ],
      "metadata": {
        "id": "VUYvppbAcPNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Obteniendo el id"
      ],
      "metadata": {
        "id": "dJKoHXM9d5op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Para cada palabra se genera un id único.\n",
        "# En este caso mostramos el id y la palabra relacionada\n",
        "print([(word.orth, word.orth_) for word in text_doc])\n"
      ],
      "metadata": {
        "id": "Z4Nx0YVkczvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Obeniendo el tensor\n",
        "\n",
        "Cada palabra tiene asociado un Tensor, su representación es mediante un objeto NumPy array. Este Tensor posiciona la palabra en un espacio vectorial de n dimensiones."
      ],
      "metadata": {
        "id": "_1pg73XYfF0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word in text_doc:\n",
        "  print(f\"{word}: {word.tensor}\" )"
      ],
      "metadata": {
        "id": "jHLXWgfJeOvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Obteniendo la posición dentro de la sentencia"
      ],
      "metadata": {
        "id": "27V9V2Bhg0cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Con la propiedad i obtenemos la posición del token dentro de la sentencia.\n",
        "# Con la propiedad idx obtenemos la posición del caracter dentro de la sentencia.\n",
        "# El idx me permite hacer slice del texto\n",
        "print([(word, word.i, word.idx) for word in text_doc])\n",
        "\n",
        "print()\n",
        "print(\"Mostramos el slice según los primeros tokens:\")\n",
        "print(\"Sentencia: text_doc[0].idx - text_doc[1].idx: texto[text_doc[0].idx:text_doc[1].idx]\")\n",
        "print(f\"{text_doc[0].idx} - {text_doc[1].idx}: {texto[text_doc[0].idx:text_doc[1].idx]}\")\n",
        "print()\n",
        "print(\"Sentencia: text_doc[1].idx - text_doc[2].idx: texto[text_doc[1].idx:text_doc[2].idx]\")\n",
        "print(f\"{text_doc[1].idx} - {text_doc[2].idx}: {texto[text_doc[1].idx:text_doc[2].idx]}\")\n"
      ],
      "metadata": {
        "id": "A8ARaPxngxph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Obteniendo la entidad o ner (name enetity recognition)\n",
        "\n",
        "Identifica y clasifica nombres de entidades en un texto en categorías predefinidas, como nombres de personas, organizaciones, lugares, fechas, cantidades, entre otros.\n",
        "\n",
        "El objetivo del NER es extraer información relevante del texto, identificando las entidades nombradas y clasificándolas según su tipo, para luego poder analizarlas y utilizarlas en diferentes aplicaciones, como la extracción de información, la recuperación de información, la traducción automática, la resumen de textos, entre otros.\n",
        "\n",
        "Por ejemplo, si se tiene un texto que menciona el nombre \"Steve Jobs\", un NER identificaría que \"Steve Jobs\" es una entidad nombrada de tipo \"persona\". Si el texto también menciona el nombre de una empresa como \"Apple\", el NER identificaría que \"Apple\" es una entidad nombrada de tipo \"organización\".\n"
      ],
      "metadata": {
        "id": "nuIrxosChHax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Para este modelo sencillo podemos ver que solo identifica MISC y PER\n",
        "print([(word, word.ent_type_) for word in text_doc])"
      ],
      "metadata": {
        "id": "0uKtbT1OhH5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Obteniendo el lemma (lemmatizer)\n",
        "\n",
        "Convierte las palabras en su forma base, conocida como \"lema\". El lema es la forma de la palabra que se encuentra en un diccionario y representa su significado principal.\n",
        "\n",
        "El proceso de lematización es similar a la reducción de una palabra a su raíz, pero en lugar de simplemente eliminar los sufijos y prefijos, el lematizador utiliza un conocimiento más profundo del idioma para identificar la forma base correcta de una palabra en función del contexto en el que se utiliza.\n",
        "\n",
        "Por ejemplo:\n",
        "\n",
        "| Palabra | Lemma |\n",
        "| --- | --- |\n",
        "| corriendo | correr |\n",
        "| corre | correr |\n",
        "| cantando | cantar |\n",
        "| comiendo | comer |\n",
        "| gatos | gato |\n",
        "| mejores | mejor |\n",
        "| hablando | hablar |\n",
        "| felicidad | felicidad |\n",
        "| leyendo | leer |\n",
        "| bailando | bailar |\n",
        "| trabajadores | trabajador |\n",
        "\n",
        "`Spacy no posee con stem o stemming, lo vamos a ver con nltk`.\n"
      ],
      "metadata": {
        "id": "0jLEwBMwh-St"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print([(word, word.lemma_) for word in text_doc])"
      ],
      "metadata": {
        "id": "wQT7OALvh-tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Obteniendo la normalización del token\n",
        "\n"
      ],
      "metadata": {
        "id": "H2YlkWEFAzGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print([(word, word.norm_) for word in text_doc])"
      ],
      "metadata": {
        "id": "pf9Dv18XkODs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Obteniendo el prefijo o sufijo\n",
        "\n",
        "El prefijo viene configurado por defecto para obtener el primer caracter.\n",
        "\n",
        "El sufijo viene configurado por defecto para obtener los últimos 3 caracter."
      ],
      "metadata": {
        "id": "ACbfNB_zA8mP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print([(word, word.prefix_, word.suffix_) for word in text_doc])"
      ],
      "metadata": {
        "id": "RVTrgSOvkkPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [Análisis sintáctico o part of speech](https://universaldependencies.org/u/pos/)\n",
        "\n"
      ],
      "metadata": {
        "id": "hwz8stYbDyIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print([(word, word.pos_) for word in text_doc])"
      ],
      "metadata": {
        "id": "Hl6Pug8bm5zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "displacy.render(text_doc, style=\"dep\", jupyter=True, options={'distance': 90})"
      ],
      "metadata": {
        "id": "64_OWfyS8p-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Obteniendo el idioma del token"
      ],
      "metadata": {
        "id": "yDWEveYoFCpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print([(word, word.lang_) for word in text_doc])"
      ],
      "metadata": {
        "id": "8FgbPW1EnXTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Obteniendo la posición\n",
        "\n",
        "La propiedad i me da la posición de la palabra dentro de la lista de tokens.\n",
        "\n",
        "La propiedad idx me da la posición del caracter donde comienza la palabra."
      ],
      "metadata": {
        "id": "5U8u9pNrFJ1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print([(word, word.i, word.idx) for word in text_doc])"
      ],
      "metadata": {
        "id": "EmN3Ur2dncBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Obteniendo la probabilidad del sentimiento"
      ],
      "metadata": {
        "id": "vZ5luqaWFi_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print([(word, word.sentiment) for word in text_doc])"
      ],
      "metadata": {
        "id": "wZM13NQ1Xx5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extensiones\n",
        "\n",
        "Las extensiones me permite agregar, quitar, verificar o sobre escribir funciones o propiedades de un token."
      ],
      "metadata": {
        "id": "a_pvdvDKXHGY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Agregando funciones"
      ],
      "metadata": {
        "id": "xnWVv625X9kE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos el objeto Token\n",
        "from spacy.tokens import Token\n",
        "\n",
        "# definimos uns función que me retorna True si es una preposición\n",
        "def is_preposition(token: Token):\n",
        "  # transforme el texto del token a lower case y verifico si está dentro de\n",
        "  # la lista de presposiciones.\n",
        "  # En este caso con el guión bajo \"_\" accedo a las extensiones\n",
        "  return token.lower_ in token._.PREPOSITIONS\n",
        "\n",
        "# Agregamos una propiedad al token que es una lista con todas la preposiciones\n",
        "Token.set_extension(\"PREPOSITIONS\", default=[\n",
        "    \"a\", \"ante\", \"bajo\", \"cabe\", \"con\", \"contra\", \"de\", \"desde\", \"durante\",\n",
        "    \"en\", \"entre\", \"hacia\", \"hasta\", \"mediante\", \"para\", \"por\", \"según\", \"sin\",\n",
        "    \"so\", \"sobre\", \"tras\", \"versus\", \"vía\"])\n",
        "\n",
        "# Agregmos una función que verifica si es una preposición\n",
        "Token.set_extension(\"is_preposition\",getter=is_preposition)"
      ],
      "metadata": {
        "id": "t2QSlNPPXDZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Utilizando extensiones"
      ],
      "metadata": {
        "id": "7hiCF0-Hfx1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# t = nlp_es_sm(texto)\n",
        "print(f\"El token a verificar es ({text_doc[0]}): \" , text_doc[0]._.is_preposition)\n",
        "print(f\"El token a verificar es ({text_doc[7]}): \" , text_doc[7]._.is_preposition)"
      ],
      "metadata": {
        "id": "Gh5vR5s_b_Q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Obteniendo funciones"
      ],
      "metadata": {
        "id": "bX_dtR6YYRPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# retorna una tupla con los valores de (default, method, getter, setter)\n",
        "print(\"Está declarada la extesión 'is_preposition': \", Token.get_extension(\"is_preposition\"))\n",
        "print(\"Está declarada la extesión 'PREPOSITIONS': \", Token.get_extension(\"PREPOSITIONS\"))\n",
        "try:\n",
        "  print(\"Está declarada la extesión 'is_prepositio' (NO EXISTE): \", Token.get_extension(\"is_prepositio\"))\n",
        "except KeyError:\n",
        "  print(\"La documentación dice que tiene que lanzar una excepción\")"
      ],
      "metadata": {
        "id": "BBR8U3tqYWnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Verificando funciones"
      ],
      "metadata": {
        "id": "4sgXLmCGYZ3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Está declarada la extesión 'is_preposition': \", Token.has_extension(\"is_preposition\"))\n",
        "print(\"Está declarada la extesión 'PREPOSITIONS': \", Token.has_extension(\"PREPOSITIONS\"))"
      ],
      "metadata": {
        "id": "42XO7qQDYdj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Quitando fundiones"
      ],
      "metadata": {
        "id": "MZ_2pj6LYKH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# retorna una tupla con los valores de (default, method, getter, setter)\n",
        "# Quitamos las extensiones que definimos anteriormente\n",
        "print(\"Quitando la extensión 'is_preposition': \", Token.remove_extension(\"is_preposition\"))\n",
        "print(\"Quitando la extensión 'PREPOSITIONS': \", Token.remove_extension(\"PREPOSITIONS\"))"
      ],
      "metadata": {
        "id": "VcHuPnREYO7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "en7bz87Sh-yN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funciones de un token\n",
        "\n",
        "Tener en cuenta que trabajan de acuerdo al `análisis sintáctico`."
      ],
      "metadata": {
        "id": "5oEzNo1fDYIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_doc)\n",
        "\n",
        "print()\n",
        "print(\"Obtiene los vecinos. Por defecto trae 1 vecino.\")\n",
        "print(\"Sentencia: text_doc[7].nbor(1)\")\n",
        "print(f\"El token a verificar es ({text_doc[7]}): \" , text_doc[7].nbor(1))\n",
        "\n",
        "print()\n",
        "print(\"Trae los vecinos a la izquierda.\")\n",
        "print(\"Sentencia: list(text_doc[12].lefts)\")\n",
        "print(f\"El token a verificar es ({text_doc[12]}): \" , list(text_doc[12].lefts))\n",
        "\n",
        "print()\n",
        "print(\"Trae los vecinos a la derecha.\")\n",
        "print(\"Sentencia: list(text_doc[1].rights)\")\n",
        "print(f\"El token a verificar es ({text_doc[1]}): \" , list(text_doc[1].rights))\n",
        "\n",
        "print()\n",
        "print(\"\")\n",
        "print(\"Trae cuantos vecinos tiene a la izquierda.\")\n",
        "print(\"Sentencia: text_doc[16].n_lefts\")\n",
        "print(f\"El token a verificar es ({text_doc[16]}): \" , text_doc[16].n_lefts)\n",
        "\n",
        "print()\n",
        "print(\"Trae cuantos vecinos tiene a la derecha.\")\n",
        "print(\"Sentencia: text_doc[9].n_rights\")\n",
        "print(f\"El token a verificar es ({text_doc[9]}): \" , text_doc[9].n_rights)\n",
        "\n",
        "print()\n",
        "print(\"Contiene una secuencia de token desencadenantes.\")\n",
        "print(\"Sentencia: list(text_doc[1].subtree)\")\n",
        "print(f\"El token a verificar es ({text_doc[1]}): \" , list(text_doc[1].subtree))\n",
        "\n",
        "print()\n",
        "print(\"Muestra las conjunciones\")\n",
        "print(\"Sentencia: list(text_doc[60].conjuncts)\")\n",
        "print(f\"El token a verificar es ({text_doc[60]}): \" , list(text_doc[60].conjuncts))\n",
        "\n",
        "print()\n",
        "print(\"Muestra el vector del token.\")\n",
        "print(\"Sentencia: text_doc[0].vector\")\n",
        "print(f\"El token a verificar es ({text_doc[0]}): \" , text_doc[0].vector)\n",
        "\n",
        "print()\n",
        "print(\"Calcula la normalización del vector.\")\n",
        "print(\"Sentencia: text_doc[0].vector_norm\")\n",
        "print(f\"El token a verificar es ({text_doc[0]}): \" , text_doc[0].vector_norm)"
      ],
      "metadata": {
        "id": "yGiwD6TPDiCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Span\n",
        "\n",
        "Cuando hacemos un slice de un Doc obtenemos un objeto Span que posee el conjunto de token del slice.\n",
        "\n",
        "El Span es un subconjunto de token.\n",
        "\n",
        "Poseen la particularidad de poseer extensiones y se trabajan de la misma forma que en los tokens pero con la particularidad que el objeto a importar para modificar es `from spacy.tokens import Span`."
      ],
      "metadata": {
        "id": "go8BvUPmKgJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spans = text_doc[5:65]\n",
        "\n",
        "type(spans)"
      ],
      "metadata": {
        "id": "CwPKRHeGKQ23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Propiedades de un Span"
      ],
      "metadata": {
        "id": "h5kXkIcyLGs0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Obteniendo el tensor del span"
      ],
      "metadata": {
        "id": "gsfrafdMgbqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Span original (spans): \\n\", spans)\n",
        "print()\n",
        "print(\"Sentencia: spans.tensor\")\n",
        "print(\"Tiene el shape: \", spans.tensor.shape)\n",
        "print(\"Posee 60 filas (una por cada token) y 96 columnas del modelo que lo genero.\")\n",
        "print(\"Obtenemos el siguiente resultado: \\n\", spans.tensor)"
      ],
      "metadata": {
        "id": "UIqjgaEtgclr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Obteniendo el número del token de donde comienza"
      ],
      "metadata": {
        "id": "EDteMX9NgjZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Span original (spans): \\n\", spans)\n",
        "print()\n",
        "print()\n",
        "print(\"Sentencia: spans.start\")\n",
        "print(\"Obtenemos el siguiente resultado: \",spans.start)"
      ],
      "metadata": {
        "id": "sa1Tty-sgjZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Obteniendo el número del token de donde finaliza"
      ],
      "metadata": {
        "id": "-cKwbEqJgjrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Span original (spans): \\n\", spans)\n",
        "print()\n",
        "print()\n",
        "print(\"Sentencia: spans.end\")\n",
        "print(\"Obtenemos el siguiente resultado: \",spans.end)"
      ],
      "metadata": {
        "id": "5zJVwTAIgjrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Obteniendo el número del caracter de donde comienza"
      ],
      "metadata": {
        "id": "X7a4ZQLtgj5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Span original (spans): \\n\", spans)\n",
        "print()\n",
        "print()\n",
        "print(\"Sentencia: spans.start_char\")\n",
        "print(\"Obtenemos el siguiente resultado: \",spans.start_char)"
      ],
      "metadata": {
        "id": "s41utEvhgj5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Obteniendo el número del caracter de donde finaliza"
      ],
      "metadata": {
        "id": "CcLyw1Ytgk70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Span original (spans): \\n\", spans)\n",
        "print()\n",
        "print()\n",
        "print(\"Sentencia: spans.end_char\")\n",
        "print(\"Obtenemos el siguiente resultado: \",spans.end_char)"
      ],
      "metadata": {
        "id": "maV4O5eEgk71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Obteniendo el texto del span"
      ],
      "metadata": {
        "id": "VBS4uM75glDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Span original (spans): \\n\", spans)\n",
        "print()\n",
        "print(\"Sentencia: spans.text\")\n",
        "print(\"Obtenemos el siguiente resultado: \\n\",spans.text)"
      ],
      "metadata": {
        "id": "W1dsC8ZJglDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Obteniendo el lemma"
      ],
      "metadata": {
        "id": "YRyb_bNrglMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Span original (spans): \\n\", spans)\n",
        "print()\n",
        "print(\"Sentencia: spans.lemma_\")\n",
        "print(\"Obtenemos el siguiente resultado: \\n\",spans.lemma_)"
      ],
      "metadata": {
        "id": "7xg3NX8QglMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Obteniendo la probabilidad del sentimiento"
      ],
      "metadata": {
        "id": "s59FJEw5glTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Span original (spans): \\n\", spans)\n",
        "print()\n",
        "print(\"Sentencia: spans.sentiment\")\n",
        "print(\"Obtenemos el siguiente resultado: \",spans.sentiment)"
      ],
      "metadata": {
        "id": "rsq8yh-0glTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funciones de un span\n",
        "\n",
        "Tener en cuenta que trabajan de acuerdo al `análisis sintáctico`.\n",
        "\n",
        "Estás son las funciones principales."
      ],
      "metadata": {
        "id": "p_VPFYyoj0iT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(spans)\n",
        "\n",
        "print()\n",
        "print(\"Obtiene los vecinos. Por defecto trae 1 vecino.\")\n",
        "print(\"Sentencia: spans[7].nbor(1)\")\n",
        "print(f\"El token a verificar es ({spans[7]}): \" , spans[7].nbor(1))\n",
        "\n",
        "print()\n",
        "print(\"Trae los vecinos a la izquierda.\")\n",
        "print(\"Sentencia: list(spans[7].lefts)\")\n",
        "print(f\"El token a verificar es ({spans[7]}): \" , list(spans[7].lefts))\n",
        "\n",
        "print()\n",
        "print(\"Trae los vecinos a la derecha.\")\n",
        "print(\"Sentencia: list(spans[7].rights)\")\n",
        "print(f\"El token a verificar es ({spans[7]}): \" , list(spans[7].rights))\n",
        "\n",
        "print()\n",
        "print(\"\")\n",
        "print(\"Trae cuantos vecinos tiene a la izquierda.\")\n",
        "print(\"Sentencia: spans[7].n_lefts\")\n",
        "print(f\"El token a verificar es ({spans[7]}): \" , spans[7].n_lefts)\n",
        "\n",
        "print()\n",
        "print(\"Trae cuantos vecinos tiene a la derecha.\")\n",
        "print(\"Sentencia: spans[7].n_rights\")\n",
        "print(f\"El token a verificar es ({spans[7]}): \" , spans[7].n_rights)\n",
        "\n",
        "print()\n",
        "print(\"Contiene una secuencia de token desencadenantes.\")\n",
        "print(\"Sentencia: list(spans[7].subtree)\")\n",
        "print(f\"El token a verificar es ({spans[7]}): \" , list(spans[7].subtree))\n",
        "\n",
        "print()\n",
        "print(\"Muestra las conjunciones\")\n",
        "print(\"Sentencia: list(spans[30].conjuncts)\")\n",
        "print(f\"El token a verificar es ({spans[30]}): \" , list(spans[30].conjuncts))\n",
        "\n",
        "print()\n",
        "print(\"Muestra el vector del token.\")\n",
        "print(\"Sentencia: spans[0].vector\")\n",
        "print(f\"El token a verificar es '{spans[0]}': \" , spans[0].vector)\n",
        "\n",
        "print()\n",
        "print(\"Calcula la normalización del vector.\")\n",
        "print(\"Sentencia: spans[0].vector_norm\")\n",
        "print(f\"El token a verificar es '{spans[0]}': \" , spans[0].vector_norm)"
      ],
      "metadata": {
        "id": "at_LyM24lI0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stopwords\n",
        "\n",
        "\n",
        "Las stop words, son palabras que se consideran muy comunes en un idioma en particular y, por lo tanto, no aportan un valor semántico significativo a una frase o texto. Ejemplos comunes de stop words en inglés son \"de\", \"la\", \"que\", \"con\", \"muy\", \"ante\", \"eso\", entre otros.\n",
        "\n",
        "En el procesamiento de lenguaje natural, las stop words se eliminan o se ignoran para evitar que afecten el resultado de la búsqueda o el análisis. La eliminación de stop words puede ayudar a reducir la dimensionalidad del texto y mejorar la eficiencia de los algoritmos de búsqueda, ya que estas palabras no proporcionan información útil para la clasificación o la recuperación de información.\n",
        "\n",
        "Sin embargo, la eliminación de stop words también puede tener un impacto negativo en el resultado final, especialmente si se trata de un texto corto o si las stop words son importantes en el contexto específico del texto. Por esta razón, en algunos casos puede ser necesario ajustar el conjunto de stop words o no eliminarlas por completo.\n",
        "\n",
        "Es importante destacar que el conjunto de stop words varía según el idioma y el contexto, por lo que es necesario elegir cuidadosamente qué palabras se eliminan o se mantienen en cada caso."
      ],
      "metadata": {
        "id": "OlnAzDksj6Kw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = nlp_es_sm.Defaults.stop_words\n",
        "print(\"En nltk teníamos: 313\")\n",
        "print(\"Existen: \", len(stopwords))\n",
        "print(\"Estos son los stop words en español:\\n\", sorted(stopwords))"
      ],
      "metadata": {
        "id": "iqjimqogj8yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vamos a limpiar el texto que venimos trabajando\n",
        "# verificamos si la palabra no esta dentro de la lista de stopwords.\n",
        "# print(\" \".join([word for word in text_doc if word not in stopwords]))\n",
        "print(\" \".join([str(word) for word in text_doc  if str(word).lower() not in stopwords]))"
      ],
      "metadata": {
        "id": "I-36lfT3kxOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objeto Matcher\n",
        "\n",
        "Basándose en expresiones regulares, me permite encontrar palabras y frases a travez de los atributos de un token. Las reglas pueden referirse a anotaciones de tokens (como el texto o las etiquetas), así como a atributos léxicos como Token.is_punct.\n",
        "\n",
        "| Forma | Tipo | Descripción |\n",
        "| --- | --- | --- |\n",
        "| ORTH | str | El texto literal exacto de un token. |\n",
        "| TEXT | str | El texto literal exacto de un token. |\n",
        "| NORM | str | La forma normalizada del texto del token. |\n",
        "| LOWER | str | La forma minúscula del texto del token. |\n",
        "| LENGTH | int | La longitud del texto. |\n",
        "| IS_ALPHA, IS_ASCII, IS_DIGIT | bool | El texto simbólico consta de caracteres alfabéticos, caracteres ASCII, dígitos. |\n",
        "| IS_LOWER, IS_UPPER, IS_TITLE | bool | El texto de la ficha está en minúsculas, mayúsculas, título. |\n",
        "| IS_PUNCT, IS_SPACE, IS_STOP | bool | La ficha contiene signos de puntuación, espacios en blanco o una palabra de parada. |\n",
        "| IS_SENT_START | bool | La entrada es el principio de la frase. |\n",
        "| LIKE_NUM, LIKE_URL, LIKE_EMAIL | bool | El texto del token se parece a un número, URL, email. |\n",
        "| SPACY | bool | El token tiene un espacio al final. |\n",
        "| POS, TAG, MORPH, DEP, LEMMA, SHAPE | str | Etiqueta de pos simple y extendida, análisis morfológico, etiqueta de dependencia, lema, forma. |\n",
        "| ENT_TYPE | str | La etiqueta de entidad del token. |\n",
        "| ENT_IOB | str | La parte IOB de la etiqueta de entidad del token. |\n",
        "| ENT_ID | str | El ID de entidad de la ficha (ent_id). |\n",
        "| ENT_KB_ID | str | El ID de la base de conocimiento de la entidad del token (ent_kb_id). |\n",
        "| OP | str | Operador o cuantificador para determinar la frecuencia de coincidencia con un patrón de token. |"
      ],
      "metadata": {
        "id": "zDQ3nTrAqZ4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_ = [word for word in text_doc]\n",
        "pos_ = [word.pos for word in text_doc]\n",
        "lemmas_ = [word.lemma_ for word in text_doc]\n",
        "!pip install tabulate\n",
        "from tabulate import tabulate\n",
        "print(tabulate([(word, word.pos_, word.lemma_) for word in text_doc], headers=[\"PALABRA\", \"POS\", \"LEMMA\"]))"
      ],
      "metadata": {
        "id": "WsMzDcgqzAcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.matcher import Matcher\n",
        "# creamos un matcher con el vocabulario del idioma\n",
        "matcher = Matcher(nlp_es_sm.vocab)\n",
        "# creamos la matcher de la expresión regular, en este caso\n",
        "# buscamos el lemma \"ser\"\n",
        "pattern = [{\"LEMMA\": \"ser\"}]\n",
        "#agregamos el patron\n",
        "matcher.add(\"lemma_ser\", [pattern])\n",
        "# ejecutamos el matcher\n",
        "matches = matcher(text_doc)\n",
        "\n",
        "for match_ in matches:\n",
        "  print(f\"La palabra correspondiente es '{text_doc[match_[1]:match_[2]]}'\")\n",
        "  print(f\"El texto alrededor de la palabra es:\\n\", text_doc[match_[1]-5:match_[2]+5])\n",
        "  print()"
      ],
      "metadata": {
        "id": "J4KCo1Lkqd5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creamos un matcher con el vocabulario del idioma\n",
        "matcher = Matcher(nlp_es_sm.vocab)\n",
        "# creamos la matcher de la expresión regular, en este caso\n",
        "# buscamos el pos \"sustantivo+conjunción+sustantivo\"\n",
        "pattern = [{\"POS\": \"NOUN\"},\n",
        "           {\"POS\": \"CCONJ\"},\n",
        "           {\"POS\": \"NOUN\"},]\n",
        "#agregamos el patron\n",
        "matcher.add(\"noun_cconj_noun\", [pattern])\n",
        "# ejecutamos el matcher\n",
        "matches = matcher(text_doc)\n",
        "\n",
        "for match_ in matches:\n",
        "  print(f\"La palabra correspondiente es '{text_doc[match_[1]:match_[2]]}'\")\n",
        "  print(f\"El texto alrededor de la palabra es:\\n\", text_doc[match_[1]-5:match_[2]+5])\n",
        "  print()"
      ],
      "metadata": {
        "id": "62P4QvttwYgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creamos un matcher con el vocabulario del idioma\n",
        "matcher = Matcher(nlp_es_sm.vocab)\n",
        "# creamos la matcher de la expresión regular, en este caso\n",
        "# buscamos la palabra \"el\"+\"sustantivo\"\n",
        "pattern = [\n",
        "    {\"LOWER\": \"el\"},\n",
        "    {\"POS\": \"NOUN\"},\n",
        "           ]\n",
        "#agregamos el patron\n",
        "matcher.add(\"el_noum\", [pattern])\n",
        "# ejecutamos el matcher\n",
        "matches = matcher(text_doc)\n",
        "\n",
        "for match_ in matches:\n",
        "  print(f\"La palabra correspondiente es '{text_doc[match_[1]:match_[2]]}'\")\n",
        "  print(f\"El texto alrededor de la palabra es:\\n\", text_doc[match_[1]-5:match_[2]+5])\n",
        "  print()"
      ],
      "metadata": {
        "id": "fbfIZUnRxYfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creamos un matcher con el vocabulario del idioma\n",
        "matcher = Matcher(nlp_es_sm.vocab)\n",
        "# creamos la matcher de la expresión regular, en este caso\n",
        "# buscamos algo que tenga la forma de 4 números\n",
        "pattern = [{\"SHAPE\": \"dddd\"}]\n",
        "#agregamos el patron\n",
        "matcher.add(\"shape_dddd\", [pattern])\n",
        "# ejecutamos el matcher\n",
        "matches = matcher(text_doc)\n",
        "\n",
        "for match_ in matches:\n",
        "  print(f\"La palabra correspondiente es '{text_doc[match_[1]:match_[2]]}'\")\n",
        "  print(f\"El texto alrededor de la palabra es:\\n\", text_doc[match_[1]-5:match_[2]+5])\n",
        "  print()"
      ],
      "metadata": {
        "id": "BWOdACqiy0KX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Otros objetos\n",
        "\n",
        "Esiten otros objetos que me permite cambiar el flujo de trabajo de acuerdo a nuevas reglas impuestras por el usuario.\n",
        "\n",
        "Por ejemplo, escribir nuevos lemmas o entidades de reconocimiento o cambios morfológicos en el reconocimiento de la oración, etc.\n",
        "\n",
        "[Ver la documentación](https://spacy.io/api)"
      ],
      "metadata": {
        "id": "H1uoBqWXs-zj"
      }
    }
  ]
}