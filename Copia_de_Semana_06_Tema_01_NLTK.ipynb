{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnaliaLeyez/AnaliaLeyez/blob/main/Copia_de_Semana_06_Tema_01_NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [nltk](https://www.nltk.org/)\n",
        "\n",
        "[Libro](https://www.nltk.org/book/)\n",
        "\n",
        "Librería para el procesamiento de texto para clasificación, tokenización, stemming, etiquetado, análisis sintáctico.\n",
        "\n",
        "Permite entrenar modelos chicos que no están relacionados con transformer. Está parte no la vamos a ver.\n",
        "\n",
        "Esta librería posee dataset de ejemplos que se pueden descargar para hacer pruebas.\n",
        "\n",
        "Todavia es muy utilizada para entrenar modelos pequeños y específicos. Posee librerías y herramientas que se utilizan en la actualidad."
      ],
      "metadata": {
        "id": "NYUSEvVx1Fs7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalación"
      ],
      "metadata": {
        "id": "pkCTAjVG4CGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U nltk"
      ],
      "metadata": {
        "id": "H888ITrZ4HZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Versión"
      ],
      "metadata": {
        "id": "EI5uvpRI4bHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KaEL56zK4dSI",
        "outputId": "89d30e09-c5d8-4e95-cd34-ba9db138975c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.9.1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuración\n",
        "\n",
        "Por defecto viene configurada para trabajar en inglés por lo tanto debemos pasarla a español.\n",
        "\n",
        "Las bibliotecas se descargan y para ello debemos ejecutar la sentencia\n",
        "```python\n",
        "nltk.download(...)\n",
        "```"
      ],
      "metadata": {
        "id": "P1Wy_Att5SqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "language = 'spanish'"
      ],
      "metadata": {
        "id": "bDJGak8H5buL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Tokenizador y tokens](https://www.nltk.org/api/nltk.tokenize.html)\n",
        "\n",
        "Un tokenizador es una herramienta que se utiliza en el procesamiento del lenguaje natural para dividir un texto en palabras individuales o en frases más pequeñas llamadas tokens. El proceso de tokenización implica la separación de signos de puntuación y caracteres especiales y la separación de las palabras en un texto.\n",
        "\n",
        "Existen varios tipos de tokenizadores, algunos dividen las palabras en sub palabras.\n"
      ],
      "metadata": {
        "id": "izDO1Y2a3L5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Para poder utilizar los tokenizadores necesitamos descargar la biblioteca\n",
        "# de caracteres de puntuación\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "57N-S1mw5OFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texto = \"\"\"La inteligencia artificial (IA), en el contexto de las ciencias de la\n",
        "computación, es el conjunto de sistemas o combinación de algoritmos, cuyo\n",
        "propósito es crear máquinas que imitan la inteligencia humana para realizar\n",
        "tareas y pueden mejorar conforme la información que recopilan. La inteligencia\n",
        "artificial no tiene como finalidad reemplazar a los humanos, sino mejorar\n",
        "significativamente las capacidades y contribuciones humanas. Se hizo presente\n",
        "poco después de la Segunda Guerra Mundial, y el nombre lo acuñó en 1956 el\n",
        "informático John McCarthy, en la Conferencia de Dartmouth.\"\"\"\n",
        "# Fijense como separó las palabras, los caracteres especiales tienen su propio\n",
        "# lugar en la lista, no están pegadas a las palabras.\n",
        "nltk.tokenize.word_tokenize(text=texto, language=language)"
      ],
      "metadata": {
        "id": "JzSklBKs3_io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# En este caso vamos a utilizar un tokenizer por oraciones. Utiliza un modelo\n",
        "# para realizar la separación.\n",
        "punktSentenceTokenizer = nltk.tokenize.PunktSentenceTokenizer()\n",
        "punktSentenceTokenizer.tokenize(text=texto)"
      ],
      "metadata": {
        "id": "AZv0tvR07p5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# En este caso vamos a utilizar un tokenizer por experesiones regulares donde\n",
        "# me quedo con las palabras que comienzan en mayúscula.\n",
        "regexpTokenizer = nltk.tokenize.RegexpTokenizer(r\"[A-Z]\\w+\")\n",
        "regexpTokenizer.tokenize(text=texto)"
      ],
      "metadata": {
        "id": "o5NUbr_c899R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# similar a \"word_tokenize\" con la diferencia que no necesito pasarle\n",
        "# el idioma porque separa por palabras o caracteres de puntuación\n",
        "toktokTokenizer = nltk.tokenize.ToktokTokenizer()\n",
        "lista_toktokTokenizer = toktokTokenizer.tokenize(text=texto)\n",
        "lista_toktokTokenizer"
      ],
      "metadata": {
        "id": "a4LRwpCA_7uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## n-gramas\n",
        "\n",
        "Se utilizan en el procesamiento del lenguaje natural para la predicción de palabras y la identificación de patrones en el texto. Los unigramas son secuencias de una sola palabra, los bigramas son secuencias de dos palabras y los trigramas son secuencias de tres palabras. Los n-gramas se utilizan en la modelización del lenguaje para predecir la probabilidad de una palabra dada una secuencia de palabras anteriores."
      ],
      "metadata": {
        "id": "xObTFFcNFzhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import bigrams\n",
        "# Vamos a tomar como ejemplo la lista de salida toktoktokenizer y vamos a generar\n",
        "# bigramas es decir vamos a generar tuplas de 2 palabras.\n",
        "# Estas funciones retornan un generador y la forma más sencilla de ver el\n",
        "# resultado es pasandola a una lista\n",
        "lista_bigrams = list(bigrams(lista_toktokTokenizer))\n",
        "lista_bigrams\n"
      ],
      "metadata": {
        "id": "1K6OjTB3J21p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import trigrams\n",
        "# Vamos a tomar como ejemplo la lista de salida toktoktokenizer y vamos a generar\n",
        "# trigramas es decir vamos a generar tuplas de 3 palabras.\n",
        "lista_trigrams = list(trigrams(lista_toktokTokenizer))\n",
        "lista_trigrams"
      ],
      "metadata": {
        "id": "SBkJeTfxMDyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import everygrams\n",
        "# Con la función everygrams podemos generar todo tipo de n-gramas que van desde\n",
        "# un máximo hasta un mínimo\n",
        "lista_everygrams = list(everygrams(lista_toktokTokenizer,min_len=1, max_len=3))\n",
        "lista_everygrams"
      ],
      "metadata": {
        "id": "msaPJz-6MXgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [POS (part of speech)](https://www.nltk.org/book/ch05.html)\n",
        "\n",
        "Por el momento solo está disponible para inglés o ruso, pero lo vamos a probar igual.\n",
        "\n",
        "VER CON [SPACY](https://colab.research.google.com/drive/18tx1BgPYxg0oONRV3uQF-s19rmjCdr9S#scrollTo=hwz8stYbDyIB)\n",
        "\n",
        "`Tener en cuenta que los ejemplos están en inglés`\n",
        "\n",
        "- Clasificación universal\n",
        "\n",
        "| Tag | Meaning | English Examples |\n",
        "| --- | --- | --- |\n",
        "| ADJ\t| adjective | new, good, high, special, big, local |\n",
        "| ADP | adposition | on, of, at, with, by, into, under |\n",
        "| ADV | adverb | really, already, still, early, now |\n",
        "| CONJ | conjunction | and, or, but, if, while, although |\n",
        "| DET | determiner, article | the, a, some, most, every, no, which |\n",
        "| NOUN | noun | year, home, costs, time, Africa |\n",
        "| NUM | numeral | twenty-four, fourth, 1991, 14:24 |\n",
        "| PRT | particle | at, on, out, over per, that, up, with |\n",
        "| PRON | pronoun | he, their, her, its, my, I, us |\n",
        "| VERB | verb | is, say, told, given, playing, would |\n",
        "| . | punctuation marks | . , ; ! |\n",
        "| X | other | ersatz, esprit, dunno, gr8, univeristy |\n"
      ],
      "metadata": {
        "id": "_yn4ws9KAYQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# descargamos la biblioteca de tageos\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "id": "Pj3u5d10ApqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# utilizamos el tokenizador TokTokTokenizer y luego le pasamos el pos_tag\n",
        "nltk.pos_tag(toktokTokenizer.tokenize(text=texto))"
      ],
      "metadata": {
        "id": "s4oYYyaCAbDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Stem o Stemming](https://www.nltk.org/api/nltk.stem.html)\n",
        "\n",
        "Es un proceso de reducción de palabras a su raíz o base común, eliminando sufijos y prefijos para obtener una forma básica de la palabra. Esta técnica es comúnmente utilizada en el procesamiento de lenguaje natural para mejorar la precisión de las búsquedas y la recuperación de información en los motores de búsqueda y otras aplicaciones similares.\n",
        "\n",
        "Por ejemplo:\n",
        "\n",
        "El objetivo del stemming es reducir las palabras a su forma más básica, para que se puedan agrupar y contar con mayor facilidad. Esto puede ser útil para análisis de texto, minería de datos y otras aplicaciones en las que es importante contar y agrupar palabras de manera eficiente.\n",
        "\n",
        "Hay varios algoritmos de stemming, como el algoritmo de Porter, el algoritmo Snowball, el algoritmo Lancaster, entre otros. Cada algoritmo tiene sus propias reglas para reducir las palabras a su raíz común, y puede haber variaciones en el resultado dependiendo del algoritmo utilizado y de la lengua del texto a procesar.\n",
        "\n",
        "Por ejemplo:\n",
        "\n",
        "| Palabra | SnowballStemmer |\n",
        "| --- | --- |\n",
        "| corriendo | corr |\n",
        "| corre | corr |\n",
        "| cantando | cant |\n",
        "| comiendo | com |\n",
        "| gatos | gat |\n",
        "| mejores | mejor |\n",
        "| hablando | habl |\n",
        "| felicidad | felic |\n",
        "| leyendo | leyend |\n",
        "| bailando | bail |\n",
        "| trabajadores | trabaj |"
      ],
      "metadata": {
        "id": "2sx0a3GoEeta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowballStemmer = SnowballStemmer('spanish')\n",
        "\n",
        "print(\"Los idiomas soportados por Snowball son:\\n\", snowballStemmer.languages)\n",
        "print()\n",
        "# utilizamos el tokenizador TokTokTokenizer y luego le pasamos el pos_tag\n",
        "print([(word, snowballStemmer.stem(word)) for word in toktokTokenizer.tokenize(text=texto)])"
      ],
      "metadata": {
        "id": "fi2oAvz4YLMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemma o Lemmatizer\n",
        "\n",
        "Convierte las palabras en su forma base, conocida como \"lema\". El lema es la forma de la palabra que se encuentra en un diccionario y representa su significado principal.\n",
        "\n",
        "El proceso de lematización es similar a la reducción de una palabra a su raíz, pero en lugar de simplemente eliminar los sufijos y prefijos, el lematizador utiliza un conocimiento más profundo del idioma para identificar la forma base correcta de una palabra en función del contexto en el que se utiliza.\n",
        "\n",
        "Por ejemplo:\n",
        "\n",
        "| Palabra Español | Palabra inglés | Lemma |\n",
        "| --- | --- | --- |\n",
        "| corriendo | running | running |\n",
        "| corre | running | running |\n",
        "| cantando | singing | singing |\n",
        "| comiendo | eating | eating |\n",
        "| gatos | cats | cat |\n",
        "| mejores | best | best |\n",
        "| hablando | talking | talking |\n",
        "| felicidad | happiness | happiness |\n",
        "| leyendo | reading | reading |\n",
        "| bailando | dancing | dancing |\n",
        "| trabajadores | workers | worker |\n",
        "\n",
        "`Ver el ejemplo en Spacy. La librería wordnet no es lo suficientemente robusta y no funciona para español`.\n",
        "\n",
        "[Link a Spacy](https://colab.research.google.com/drive/18tx1BgPYxg0oONRV3uQF-s19rmjCdr9S#scrollTo=0jLEwBMwh-St&line=1&uniqifier=1)"
      ],
      "metadata": {
        "id": "Po_A4LUFXp9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# para el lemma necesitamos descargar\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "doHJZRRnbyWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "wordNetLemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# utilizamos el tokenizador TokTokTokenizer y luego le pasamos el lemma\n",
        "# no observamos cambios por el idioma\n",
        "print([(word, wordNetLemmatizer.lemmatize(word)) for word in toktokTokenizer.tokenize(text=texto)])"
      ],
      "metadata": {
        "id": "P5qx0A7rYVNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stop words\n",
        "\n",
        "Las stop words, son palabras que se consideran muy comunes en un idioma en particular y, por lo tanto, no aportan un valor semántico significativo a una frase o texto. Ejemplos comunes de stop words en inglés son \"de\", \"la\", \"que\", \"con\", \"muy\", \"ante\", \"eso\", entre otros.\n",
        "\n",
        "En el procesamiento de lenguaje natural, las stop words se eliminan o se ignoran para evitar que afecten el resultado de la búsqueda o el análisis. La eliminación de stop words puede ayudar a reducir la dimensionalidad del texto y mejorar la eficiencia de los algoritmos de búsqueda, ya que estas palabras no proporcionan información útil para la clasificación o la recuperación de información.\n",
        "\n",
        "Sin embargo, la eliminación de stop words también puede tener un impacto negativo en el resultado final, especialmente si se trata de un texto corto o si las stop words son importantes en el contexto específico del texto. Por esta razón, en algunos casos puede ser necesario ajustar el conjunto de stop words o no eliminarlas por completo.\n",
        "\n",
        "Es importante destacar que el conjunto de stop words varía según el idioma y el contexto, por lo que es necesario elegir cuidadosamente qué palabras se eliminan o se mantienen en cada caso.\n",
        "\n",
        "[Link a Spacy](https://colab.research.google.com/drive/18tx1BgPYxg0oONRV3uQF-s19rmjCdr9S#scrollTo=OlnAzDksj6Kw&line=2&uniqifier=1)"
      ],
      "metadata": {
        "id": "s0Qfd6xac3Vk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# para el lemma necesitamos descargar\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "p3vFTzHxgeS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = nltk.corpus.stopwords.words('spanish')\n",
        "# para ver todos los stop words desc\n",
        "print(\"Existen: \", len(stopwords))\n",
        "print(\"Estos son los stop words en español:\\n\", stopwords)"
      ],
      "metadata": {
        "id": "M1DpAmFadlXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vamos a limpiar el texto que venimos trabajando\n",
        "# verificamos si la palabra no esta dentro de la lista de stopwords.\n",
        "print(\" \".join([word for word in toktokTokenizer.tokenize(text=texto) if word.lower() not in stopwords]))"
      ],
      "metadata": {
        "id": "dzM9ASF6hVpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CODCCb8Oph96"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}